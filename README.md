# Too Long; Don't Read.
# A Paper List of [Mingrui CHEN](https://github.com/Walnutes/TL-DR)

Topics:
- Learning
  - [3D Vision](topics/3dvision.md)
  - [Vision Backbone](topics/backbone.md)
  - [Generative Model](topics/generative_model.md)
  - [Diffusion Model](topics/diffusion_model.md)
  - [Large Language Model](topics/llm.md)
  - [Self-Supervised Learning](topics/self_supervised_learning.md)

- [Miscellaneous](topics/misc.md)

Confs:
- 2023
  - [ICLR 2024 submission](https://openreview.net/group?id=ICLR.cc/2024/Conference) / [scores](https://guoqiangwei.xyz/iclr2024_stats/iclr2024_submissions.html)
  - [NeurIPS 2023](https://neurips.cc/virtual/2023/papers.html)
  - [ICCV 2023](https://openaccess.thecvf.com/ICCV2023?day=all)
  - [ICML 2023](https://icml.cc/virtual/2023/papers.html?filter=titles)
  - [CVPR 2023](https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers)
  - [ICLR 2023](https://iclr.cc/virtual/2023/papers.html?filter=titles)
- 2022
  - [NeurIPS 2022](https://neurips.cc/virtual/2022/papers.html?filter=titles)
  - [ICLR2023](https://openreview.net/group?id=ICLR.cc/2023/Conference)
  - [ECCV2022](https://eccv2022.ecva.net/program/accepted-papers/)
  - [ICML2022](https://dblp.org/db/conf/icml/icml2022.html) 


# Recent Paper
- ICLR 2023 outstanding paper honorable mentions, Disentanglement with Biological Constraints: A Theory of Functional Cell Types, [Openreview](https://openreview.net/forum?id=9Z_GfhZnGH)
- ICCV 2023 oral, Tracking Everything Everywhere All at Once, [Website](https://omnimotion.github.io/)
- arXiv 2023.06, **DreamSim**: Learning New Dimensions of Human Visual Similarity using Synthetic Data, [arXiv](https://arxiv.org/abs/2306.09344) / [Website](https://dreamsim-nights.github.io/)
- ICCV 2023, Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models, [Website](https://energy-based-model.github.io/unsupervised-concept-discovery/)
- AAAI 2018, **FiLM**: Visual Reasoning with a General Conditioning Layer, [arXiv](https://arxiv.org/abs/1709.07871)
- CVPR 2017 oral, **Network Dissection**: Quantifying Interpretability of Deep Visual Representations, [arXiv](https://arxiv.org/abs/1704.05796) / [Website](http://netdissect.csail.mit.edu/)
- CVPR 2023 award candidate, Data-driven Feature Tracking for Event Cameras, [arXiv](https://arxiv.org/abs/2211.12826)
- CVPR 2023 award candidate, What Can Human Sketches Do for Object Detection?, [Website](http://www.pinakinathc.me/sketch-detect/)
- CVPR 2023 award candidate, Visual Programming for Compositional Visual Reasoning, [Website](https://prior.allenai.org/projects/visprog)
- CVPR 2023 award candidate, On Distillation of Guided Diffusion Models, [arXiv](https://arxiv.org/abs/2210.03142)
- CVPR 2023 award candidate, **OmniObject3D**: Large Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation, [Website](https://omniobject3d.github.io/)
- CVPR 2023, **ULIP**: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding, [arXiv](https://arxiv.org/abs/2212.05171) / [Github](https://github.com/salesforce/ULIP)
- CVPR 2023, Learning Video Representations from Large Language Models, [Website](https://facebookresearch.github.io/LaViLa/) / [Github](https://github.com/facebookresearch/LaViLa)
- CVPR 2023, **PLA**: Language-Driven Open-Vocabulary 3D Scene Understanding, [Website](https://dingry.github.io/projects/PLA)
- arXiv 2023, Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, [arXiv](https://arxiv.org/abs/2303.05499) / [GitHub](https://github.com/IDEA-Research/GroundingDINO)
- arXiv 2023, Zero-1-to-3: Zero-shot One Image to 3D Object, [arXiv](https://arxiv.org/abs/2303.11328)
- ICLR 2023, Towards Stable Test-Time Adaptation in Dynamic Wild World, [arXiv](https://arxiv.org/abs/2302.12400)
- CVPR 2023 highlight, Neural Volumetric Memory for Visual Locomotion Control, [Website](https://rchalyang.github.io/NVM/)
- arXiv 2023, Segment Anything, [Website](https://segment-anything.com/)
- ICRA 2023, DribbleBot: Dynamic Legged Manipulation in the Wild, [Website](https://gmargo11.github.io/dribblebot/)
- arXiv 2023, Alpaca: A Strong, Replicable Instruction-Following Model, [Website](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- arXiv 2023, VC-1: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?, [Website](https://eai-vc.github.io/)

# Contact
If you have any questions or suggestions, please feel free to contact me at charmier2003@gmail.com .
